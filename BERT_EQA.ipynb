{
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z5JDlV4gRS9",
        "outputId": "6274f5db-0871-407c-af46-ad5e76721920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "âœ… NVIDIA GPU detected!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "from evaluate import load\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Transformers and datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DefaultDataCollator,\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Check device - optimized for Apple Silicon\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device for training\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Additional device info\n",
        "if device.type == \"mps\":\n",
        "    print(\"Apple Silicon\")\n",
        "elif device.type == \"cuda\":\n",
        "    print(\"NVIDIA GPU\")\n",
        "elif device.type == \"cpu\":\n",
        "    print(\"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load local squad data from canvas\n",
        "def load_squad_data(file_path):\n",
        "    \"\"\"Load SQuAD format JSON data\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# load training and validation data\n",
        "train_data = load_squad_data('train-v1.1.json')\n",
        "dev_data = load_squad_data('dev-v1.1.json')\n",
        "\n",
        "print(f\"Training data version: {train_data['version']}\")\n",
        "print(f\"Number of articles in training set: {len(train_data['data'])}\")\n",
        "print(f\"Number of articles in dev set: {len(dev_data['data'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PU_6tIdgndj",
        "outputId": "a829bf03-5eae-4f32-e78c-a7593c91c247"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data version: 1.1\n",
            "Number of articles in training set: 442\n",
            "Number of articles in dev set: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_squad_data(squad_data):\n",
        "    \"\"\"Parse SQuAD data into a flat list of examples\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for article in squad_data['data']:\n",
        "        title = article['title']\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                example = {\n",
        "                    'id': qa['id'],\n",
        "                    'title': title,\n",
        "                    'context': context,\n",
        "                    'question': qa['question'],\n",
        "                    'answers': qa['answers']\n",
        "                }\n",
        "                examples.append(example)\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Parse the data\n",
        "train_examples = parse_squad_data(train_data)\n",
        "dev_examples = parse_squad_data(dev_data)\n",
        "\n",
        "print(f\"\\nNumber of training examples: {len(train_examples)}\")\n",
        "print(f\"Number of dev examples: {len(dev_examples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYaVGY5Kg2W2",
        "outputId": "0ef05308-feb7-473b-d58e-b0757dc9d8f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of training examples: 87599\n",
            "Number of dev examples: 10570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explore sample\n",
        "sample = train_examples[0]\n",
        "print(\"Sample Training Example:\")\n",
        "print(f\"ID: {sample['id']}\")\n",
        "print(f\"Title: {sample['title']}\")\n",
        "print(f\"\\nQuestion: {sample['question']}\")\n",
        "print(f\"\\nContext (first 200 chars): {sample['context'][:200]}...\")\n",
        "print(f\"\\nAnswers: {sample['answers']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc2BTemYg4RV",
        "outputId": "0e641ba2-47bc-4cb6-fd32-fab5536ea856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Training Example:\n",
            "ID: 5733be284776f41900661182\n",
            "Title: University_of_Notre_Dame\n",
            "\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "\n",
            "Context (first 200 chars): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
            "\n",
            "Answers: [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute statistics\n",
        "def compute_statistics(examples):\n",
        "    \"\"\"Compute various statistics about the dataset\"\"\"\n",
        "    stats = {\n",
        "        'num_examples': len(examples),\n",
        "        'question_lengths': [],\n",
        "        'context_lengths': [],\n",
        "        'answer_lengths': [],\n",
        "        'num_answers': []\n",
        "    }\n",
        "\n",
        "    for ex in examples:\n",
        "        stats['question_lengths'].append(len(ex['question'].split()))\n",
        "        stats['context_lengths'].append(len(ex['context'].split()))\n",
        "        stats['num_answers'].append(len(ex['answers']))\n",
        "        if ex['answers']:\n",
        "            stats['answer_lengths'].append(len(ex['answers'][0]['text'].split()))\n",
        "\n",
        "    return stats\n",
        "\n",
        "train_stats = compute_statistics(train_examples)\n",
        "dev_stats = compute_statistics(dev_examples)\n",
        "\n",
        "print(\"Training Set Statistics:\")\n",
        "print(f\"  Total examples: {train_stats['num_examples']}\")\n",
        "print(f\"  Avg question length: {np.mean(train_stats['question_lengths']):.2f} words\")\n",
        "print(f\"  Avg context length: {np.mean(train_stats['context_lengths']):.2f} words\")\n",
        "print(f\"  Avg answer length: {np.mean(train_stats['answer_lengths']):.2f} words\")\n",
        "print(f\"\\nDev Set Statistics:\")\n",
        "print(f\"  Total examples: {dev_stats['num_examples']}\")\n",
        "print(f\"  Avg question length: {np.mean(dev_stats['question_lengths']):.2f} words\")\n",
        "print(f\"  Avg context length: {np.mean(dev_stats['context_lengths']):.2f} words\")\n",
        "print(f\"  Avg answer length: {np.mean(dev_stats['answer_lengths']):.2f} words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a914JkJg5tM",
        "outputId": "4e021fd6-4f01-4604-fcf4-e0bcdebf24ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Statistics:\n",
            "  Total examples: 87599\n",
            "  Avg question length: 10.06 words\n",
            "  Avg context length: 119.76 words\n",
            "  Avg answer length: 3.16 words\n",
            "\n",
            "Dev Set Statistics:\n",
            "  Total examples: 10570\n",
            "  Avg question length: 10.22 words\n",
            "  Avg context length: 123.95 words\n",
            "  Avg answer length: 3.02 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize tokenizer\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# set maximum lengths\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "\n",
        "print(f\"Tokenizer: {MODEL_NAME}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Max length: {MAX_LENGTH}\")\n",
        "print(f\"Doc stride: {DOC_STRIDE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "5aec04079d0743f290089d1632f4a148",
            "d0ad34bef6b441449f8d7036a4ccac17",
            "d53d4edaef9a4535b41b7aa7d13776e8",
            "0ee3d67cd7de48b89dcde85564823288",
            "623821f3bc05499abea6b72f1a00147c",
            "76c400e4d367424babea5e39c5310bdc",
            "82cab4a7d74743e4a0515d48b9dae583",
            "5f25097ef1764bd487d57b8d3bda0a5e",
            "4fe74a355981413ca3cb5f116bf143b8",
            "76feb7c8442849c1a27245f93fe9204f",
            "fffd8a25d4244ff68108f6eff3ac77a9",
            "6eea51812a7241b6a246c307f76156eb",
            "78ca167c723d4831a0132e014d64273f",
            "b0a37d6b40194c7da73b242bef752173",
            "771cb9c1b8914ad4844c987f68eddd61",
            "8676c3eae4364ed8bcf49e59a2fe4335",
            "0631f74a13b8433c9fa983d51c0d1c9d",
            "0c25248598fb4b0da94769827cd71f85",
            "d75c40c043b54ae6b5514b7e77811c80",
            "aec2ee0459884485a9e5d41ccccc1630",
            "4c21059e86ee4dc7b25d9f22ee8efae1",
            "2f9cd651cfe340c48727f62eea50bbe7",
            "e1ae5cf1a5f24026b7c3f7950ffbe2a1",
            "9df58a342fae4d3b99bdcd576f6b6822",
            "5dff56084b83489780cfca64747ae48e",
            "380dad0c058b4ee38dd90302be47c573",
            "69d641c150414637a84b7533aa73005a",
            "bd11e5c291054f67979f9a1faf02cac7",
            "99ebe34848b349e4bafc859b0d53bd85",
            "30c491d6f88c418abd59d4c820404925",
            "1b00144f0cc44773a24bb265a0f41bc4",
            "5e1c5fbf63264419b314ab433c1e9ff7",
            "4f9ca46c01524e359b1c78f48c3851f6",
            "06d6cb56321f485ab2c3a79ebceb067d",
            "3a0af48cb320485c95a0c62c9fd99ad9",
            "d206be95ed7140ca9a8fccf37fb53bb9",
            "e3e6298d67d74838b6239c5808e31a1c",
            "5516b96a12a34041bd8c85e88094175d",
            "6db03e792efc4b0fad77c055e72b2867",
            "860a8cb88aab4775adb1439795603119",
            "250f600aeffa4a808f938761c8c2183e",
            "e0dcc94ffa5b42288b3c957f29dc707e",
            "c9e887fdd1944bb8af42d3c383a7c942",
            "41dc0a47fb20491a952eec8ac5abadc8"
          ]
        },
        "id": "FrlaR0v-g8LX",
        "outputId": "13641cbb-17d7-4a36-807f-d6468dfa7dc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aec04079d0743f290089d1632f4a148"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eea51812a7241b6a246c307f76156eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ae5cf1a5f24026b7c3f7950ffbe2a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06d6cb56321f485ab2c3a79ebceb067d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer: bert-base-uncased\n",
            "Vocabulary size: 30522\n",
            "Max length: 384\n",
            "Doc stride: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert examples to huggingface dataset format\n",
        "def examples_to_dataset(examples):\n",
        "    \"\"\"Convert parsed examples to HuggingFace Dataset\"\"\"\n",
        "    dataset_dict = {\n",
        "        'id': [],\n",
        "        'title': [],\n",
        "        'context': [],\n",
        "        'question': [],\n",
        "        'answers': []\n",
        "    }\n",
        "\n",
        "    for ex in examples:\n",
        "        dataset_dict['id'].append(ex['id'])\n",
        "        dataset_dict['title'].append(ex['title'])\n",
        "        dataset_dict['context'].append(ex['context'])\n",
        "        dataset_dict['question'].append(ex['question'])\n",
        "\n",
        "        # Format answers for HuggingFace\n",
        "        if ex['answers']:\n",
        "            dataset_dict['answers'].append({\n",
        "                'text': [ans['text'] for ans in ex['answers']],\n",
        "                'answer_start': [ans['answer_start'] for ans in ex['answers']]\n",
        "            })\n",
        "        else:\n",
        "            dataset_dict['answers'].append({'text': [], 'answer_start': []})\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# create datasets\n",
        "train_dataset = examples_to_dataset(train_examples)\n",
        "dev_dataset = examples_to_dataset(dev_examples)\n",
        "\n",
        "# create DatasetDict\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': dev_dataset\n",
        "})\n",
        "\n",
        "print(raw_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkK2ru79g-54",
        "outputId": "d6d1047c-8592-4bdf-f206-8e78c4572eba"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 87599\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 10570\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing function for training\n",
        "def prepare_train_features(examples):\n",
        "    \"\"\"\n",
        "    Tokenize questions and contexts, and find the start and end positions of answers.\n",
        "    Handle cases where the context is too long by creating multiple features with stride.\n",
        "    \"\"\"\n",
        "    # tokenize questions and contexts\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # map from feature back to original example\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # initialize labels\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # Get the input_ids to find the context span\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Get the sequence ids (0 for question, 1 for context)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Get the original example index\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If no answers, set positions to cls_index\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Get the start and end character positions of the answer\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Find the start and end token positions\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Check if the answer is in this feature (not truncated)\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Move token_start_index and token_end_index to the answer span\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "MSdcwhc_hA1e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing function for validation dataset\n",
        "def prepare_validation_features(examples):\n",
        "    \"\"\"\n",
        "    Tokenize questions and contexts for validation.\n",
        "    Keep offset mapping and example ids for post-processing.\n",
        "    \"\"\"\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Map from feature back to original example\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Keep example ids and offset mapping for post-processing\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Get the sequence ids\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Set context offsets to None\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "        # Get the original example index\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "gGCHBYy0hC7h"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing - use prepare_train_features for BOTH during training\n",
        "print(\"Tokenizing training data...\")\n",
        "tokenized_train = raw_datasets[\"train\"].map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        "    desc=\"Running tokenizer on train dataset\",\n",
        ")\n",
        "\n",
        "print(\"\\nTokenizing validation data...\")\n",
        "tokenized_validation = raw_datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        "    desc=\"Running tokenizer on validation dataset\",\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining features: {len(tokenized_train)}\")\n",
        "print(f\"Validation features: {len(tokenized_validation)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "bc8ca3f8486d4a71b2cb41ab69c9d130",
            "a563084019844ac88fe4ef20f92898a7",
            "8dce72274db04a4899ba8342b6957fca",
            "7a4126e678ee409fb21c0e428bc3fd83",
            "ec9752e1dc5b4905abee5d7ce4881eb1",
            "60c5309461e548dda3c3d0109f438686",
            "8af2c5e1cf0e4b80b420f415f9693db6",
            "ce73f1127b6740e3b338313625bff96e",
            "fceaacf9f2204a80a0c7b3ded9e9b077",
            "a64810cac1d04e628640ad6cef075735",
            "0fefd249fc134d338457cfe07910ce25",
            "f881de80fbf943f1a0806107d51cfdda",
            "f8ba328a6b5b4abfac1862c570ca4d98",
            "469df36ffca3445e97784b15e4bf54cd",
            "3a7eaa768c58466ab2edf53def0942c4",
            "7ba08a19622246f9adb56c3cfef0d0a7",
            "cc4c401085c943e28e123b183927dd79",
            "25d5150ce6954bc3be95b6ebd6cd4a43",
            "cd26ca04e9854078b27dff21930c85b3",
            "dadc147abc96438e9ca6d25d2f6ed514",
            "c61a646f3b3d4ae29b55f35faed0f81d",
            "b206f5ee2b8148fc8e5f414fccc7174e"
          ]
        },
        "id": "dz5Kl6IuhFF6",
        "outputId": "18791506-81f0-4259-aff4-c1a3dc6caa1b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing training data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on train dataset:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc8ca3f8486d4a71b2cb41ab69c9d130"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenizing validation data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on validation dataset:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f881de80fbf943f1a0806107d51cfdda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training features: 88524\n",
            "Validation features: 10784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for question answering\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "# Print model info\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "num_frozen_params = num_params - num_trainable_params\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Total parameters: {num_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWjLHaxihHQ7",
        "outputId": "50961c88-4f78-41dd-a9e8-c4da6ecad7f1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased\n",
            "Total parameters: 108,893,186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models/bert-qa-baseline\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1, # 1 epoch bc v slow\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,  # use fp16 only on CUDA\n",
        "    dataloader_pin_memory=device.type == \"cuda\",\n",
        ")\n",
        "\n",
        "\n",
        "# Data collator\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "print(\"Training Arguments:\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  FP16: {training_args.fp16}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtgNO6_2hNDY",
        "outputId": "d1b84c07-8c83-4a06-aaf3-8c5e48491030"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Arguments:\n",
            "  Batch size: 8\n",
            "  Learning rate: 3e-05\n",
            "  Epochs: 1\n",
            "  FP16: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhkCOit6hPGI",
        "outputId": "14a8a8cd-52dd-4589-e01d-41d9baf667f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer initialized successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2413387848.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"./models/bert-qa-baseline/final\")\n",
        "\n",
        "# Print training metrics\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "UvQ-zO9uhQnb",
        "outputId": "95a9ba9a-6470-4371-b871-b2994bbb0e4c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9960' max='9960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9960/9960 29:32, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.001400</td>\n",
              "      <td>0.967447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               = 14541777GF\n",
            "  train_loss               =     1.2236\n",
            "  train_runtime            = 0:29:32.51\n",
            "  train_samples_per_second =      44.95\n",
            "  train_steps_per_second   =      5.619\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# post-process predictions\n",
        "def postprocess_qa_predictions(\n",
        "    examples,\n",
        "    features,\n",
        "    predictions,\n",
        "    n_best_size=20,\n",
        "    max_answer_length=30,\n",
        "):\n",
        "    \"\"\"\n",
        "    Post-process the predictions to get the final answer text.\n",
        "    \"\"\"\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "\n",
        "    # Build a map from example to its corresponding features\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = {}\n",
        "    for i, feature in enumerate(features):\n",
        "        example_id = feature[\"example_id\"]\n",
        "        if example_id not in features_per_example:\n",
        "            features_per_example[example_id] = []\n",
        "        features_per_example[example_id].append(i)\n",
        "\n",
        "    # Dictionary to store predictions\n",
        "    predictions_dict = {}\n",
        "\n",
        "    # Loop through all examples\n",
        "    for example_index, example in enumerate(tqdm(examples, desc=\"Post-processing\")):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "\n",
        "        # Get the features for this example\n",
        "        feature_indices = features_per_example[example_id]\n",
        "\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null score\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Get top n_best_size start and end positions\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip invalid predictions\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    # Skip answers that are too long or have end < start\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    # Get the answer text\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    answer_text = context[start_char:end_char]\n",
        "\n",
        "                    valid_answers.append({\n",
        "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                        \"text\": answer_text,\n",
        "                    })\n",
        "\n",
        "        # Select the best answer\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions_dict[example_id] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions_dict"
      ],
      "metadata": {
        "id": "HXfJ9S_9hSfq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on validation set\n",
        "print(\"Getting predictions on validation set...\")\n",
        "raw_predictions = trainer.predict(tokenized_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9IKTNAsHuHRo",
        "outputId": "816755e3-5a01-40b7-d445-96e0f1ade18c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-process predictions\n",
        "final_predictions = postprocess_qa_predictions(\n",
        "    raw_datasets[\"validation\"],\n",
        "    tokenized_validation,\n",
        "    raw_predictions.predictions,\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated {len(final_predictions)} predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "6e4bee7c2638440bb2e283cf906dcdb6",
            "66c09415b55e4104862a4a513e7e9937",
            "cec0a921eb404376814e017bde4cf9af",
            "559f583439a94178b0578b7dd105b0e4",
            "a7ce0963a6d242048d6e76241541486d",
            "daf2e020ff8b4f219963d03360ad8c38",
            "954d5d064a2c4898851731ebda32cc30",
            "652462f4f69f42998486633f70e4b630",
            "70785fcde12d433bbe1fa256a2ff6420",
            "b0081dc39cd3402981e9db5e26ca4753",
            "68088b0faa4e430d838135c8d232d161"
          ]
        },
        "id": "Us1hHLqlawT7",
        "outputId": "ab82fd7a-01c2-46dc-faa0-3ed00ec21159"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Post-processing:   0%|          | 0/10570 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e4bee7c2638440bb2e283cf906dcdb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated 10570 predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to JSON file (required format for evaluate-v2.0.py)\n",
        "with open('predictions.json', 'w') as f:\n",
        "    json.dump(final_predictions, f)\n",
        "\n",
        "print(\"Predictions saved to predictions.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rj57PHnudrz",
        "outputId": "75f46b69-f27e-469a-f117-71db1c4c209e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SQuAD metric\n",
        "metric = load(\"squad\")\n",
        "\n",
        "# Format predictions and references\n",
        "formatted_predictions = [\n",
        "    {\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()\n",
        "]\n",
        "\n",
        "references = [\n",
        "    {\n",
        "        \"id\": ex[\"id\"],\n",
        "        \"answers\": {\n",
        "            \"text\": ex[\"answers\"][\"text\"],\n",
        "            \"answer_start\": ex[\"answers\"][\"answer_start\"],\n",
        "        },\n",
        "    }\n",
        "    for ex in raw_datasets[\"validation\"]\n",
        "]\n",
        "\n",
        "# Compute metrics\n",
        "results = metric.compute(predictions=formatted_predictions, references=references)\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
        "print(f\"F1 Score: {results['f1']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKLrhqZuuLRD",
        "outputId": "3aff0dc8-8ad9-4932-c715-448bf11dc61c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Exact Match: 79.45\n",
            "F1 Score: 87.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to answer questions\n",
        "def answer_question(question, context, model, tokenizer):\n",
        "    \"\"\"Answer a question given a context\"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get start and end logits\n",
        "    start_logits = outputs.start_logits[0].cpu().numpy()\n",
        "    end_logits = outputs.end_logits[0].cpu().numpy()\n",
        "\n",
        "    # Get the most likely answer\n",
        "    start_idx = np.argmax(start_logits)\n",
        "    end_idx = np.argmax(end_logits)\n",
        "\n",
        "    # Get offset mapping\n",
        "    offset_mapping = inputs[\"input_ids\"][0].cpu().numpy()\n",
        "\n",
        "    # Decode answer\n",
        "    answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx+1]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Test with a custom example\n",
        "test_context = \"\"\"\n",
        "Several commemorative events take place every year. Gatherings of thousands of people on the banks of the Vistula on Midsummerâ€™s Night for a festival called Wianki (Polish for Wreaths) have become a tradition and a yearly event in the programme of cultural events in Warsaw. The festival traces its roots to a peaceful pagan ritual where maidens would float their wreaths of herbs on the water to predict when they would be married, and to whom. By the 19th century this tradition had become a festive event, and it continues today. The city council organize concerts and other events. Each Midsummerâ€™s Eve, apart from the official floating of wreaths, jumping over fires, looking for the fern flower, there are musical performances, dignitaries' speeches, fairs and fireworks by the river bank.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"How many people gather along the banks of the Vistula for the Wianki festival?\"\n",
        "\n",
        "answer = answer_question(test_question, test_context, model, tokenizer)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q0lvxU8wyDy",
        "outputId": "9356d94a-fb8f-47f8-f059-f73f209e7b37"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How many people gather along the banks of the Vistula for the Wianki festival?\n",
            "Answer: thousands\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./bert_baseline\")"
      ],
      "metadata": {
        "id": "2sbdmN82d9XL"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bert_baseline/baseline.zip bert_baseline/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyomkM2eELl",
        "outputId": "4d528fa8-6065-422c-cadc-7c05737e1084"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bert_baseline/ (stored 0%)\n",
            "  adding: bert_baseline/config.json (deflated 47%)\n",
            "  adding: bert_baseline/model.safetensors (deflated 7%)\n",
            "  adding: bert_baseline/special_tokens_map.json (deflated 42%)\n",
            "  adding: bert_baseline/vocab.txt (deflated 53%)\n",
            "  adding: bert_baseline/tokenizer_config.json (deflated 75%)\n",
            "  adding: bert_baseline/tokenizer.json (deflated 71%)\n",
            "  adding: bert_baseline/training_args.bin (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Login once\n",
        "\n",
        "# Upload your trained model\n",
        "model.push_to_hub(\"G20-CS4248/bert-baseline-qa\")\n",
        "tokenizer.push_to_hub(\"G20-CS4248/bert-baseline-qa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "86ec77e4b8b04471837beb579a071865",
            "b531735615514a6ea28b22708cd93f37",
            "d3943026bf184a629c4ecba996a42899",
            "11fa968c174647cfb1bba8d5beab79f6",
            "4b014d36a3c540c3be58401a4d6f12e8",
            "fd48012f13a54a51839ef02d9d892337",
            "c20b52dbbf284ede87dde28afc40678d",
            "07284bf7ee7047b6a32c0b0d62eb6ce3",
            "3486be60a81f47fba68f9da651514959",
            "d4ad412fca554f8eb917a014f718ca42",
            "70f0f315f0ab4af298ee0a73045b2430",
            "7a11a18fe8f344b78b27b75083573c8d",
            "f9fc2047d70a4bc090e8c35e27e83bf0",
            "4acf34a4764d49d3a976110f77cb6176",
            "d1aefb1322d0411cbf20426ca2e026dd",
            "79fa69dee4554e21954502e3078ca11e",
            "5f9e52a09f014b039522f107b965e2f5",
            "557fab33fbd546aeb1df9d20638cedea",
            "5c5896a526f44fb986995cd8aabb370d",
            "d9a715243e80454d8ed7b636f33de994",
            "1d52d62a9e284dbca9d43d72bd0444f0",
            "d6c46b013c56499d945b707fa6f88ff4",
            "8525ee7556d8451ea80d1bb84809c9e1",
            "8ddeb3142a7d46f0973e39a47651fd67",
            "3cd94c92eaff46cea9e7e7703cc4fc8f",
            "a2736f8c759d4e9b93310f02b99b9d3c",
            "302c5794a4584140b3e1ae3a054f0ef3",
            "4e140026389946edb2411afa27f73758",
            "c5834ce14ff84d9c830926f83dd3b5b4",
            "fa616ad3add84874996a7d5dab95a30b",
            "a6085acf3b18409199a956c9b241e995",
            "17948e6195704da9aef42e14f6e12c22",
            "11aec652a6544f80b0dd095333bb2713",
            "d78ebe8816b346f7bcd7c7e2f28d6076",
            "585c0f94da0b47e98faa3bbcbc33090d",
            "2d4bf05603e24c6cbdd169f21743735e",
            "35805ca528ad419496152a65d7a0b668",
            "b55ad1c0e9bf4c39af7dfaabe8408acb",
            "3a104adf16034189942e78fbda345563",
            "8a7535850023474b91b7539bd9a5347b",
            "b7f314ba4b81453199c5a7dcdf8a178a",
            "dc661d8d9b434efa9c3d5675e5eeff64",
            "718faa0e86b549ffbea2eead93c208e3",
            "07a240eec45d4f4bb2a7cb9e3761275a",
            "1c4840a1eb8b4239a5047abb5a68331b",
            "9a8c66f9a822464d8693cf0049fd2b38",
            "de932be26bb8439bbbf4f0639d50fbd1",
            "bf642f8cc67e4fc7b6529dfc0e18821a",
            "a54381fce52c40f0bebe0e103128c46c",
            "0e3add81bdd04b4f8589d20db6f4bd6c",
            "ba7e2d8fb3a8479f9b71e69498fb94a2",
            "c3447dc2657549c0b79c626c4b005850",
            "1582bda5223d43cf90be1fa131dd64d0",
            "23945dd6294640e480b317191a686556",
            "aecd2cc0a74043d1a8f2357eb730ac1c",
            "29d5a4e7af4b48db849af54c67b894e3",
            "6c20286e729045e39a1fe5db996571c9",
            "6c9f865ca0fe49a08578c9f5634003a6",
            "1b554abbef074e07b55b5748b43b214c",
            "b426802259f446e1bd4aae856ac933b8",
            "d43095bf37a2421bb1dc839a73cbb42c",
            "6b0d5ac141734f07a4ae3681f1922b20",
            "2d1b8ab29adf42fcb12813acccccb1a5",
            "bf5dbccc5a6e44b68a79e4941b0c77f5",
            "a94811faf69845048b9960e7a9557d67",
            "56a11e47ca724cd49ae029b4f9be5382",
            "7cedc701933a445ba79fa3a7f240e201",
            "1deb36c0dd4c4a09ab9eec14a448c49b",
            "669c0e8b3b1246a8bc020449403b73e6",
            "0aee79e189244615a4fc3c7bf1776ea3"
          ]
        },
        "id": "ebXf2-GPjGEZ",
        "outputId": "ae207b3f-df04-4499-d24b-59f99360afe6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86ec77e4b8b04471837beb579a071865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "557fab33fbd546aeb1df9d20638cedea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5834ce14ff84d9c830926f83dd3b5b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...vwb6spr/model.safetensors:   0%|          | 14.2kB /  436MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a7535850023474b91b7539bd9a5347b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba7e2d8fb3a8479f9b71e69498fb94a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/G20-CS4248/bert-baseline-qa/commit/20f692dfb0c4b4afff5fd64fff3ef9b7d3096686', commit_message='Upload tokenizer', commit_description='', oid='20f692dfb0c4b4afff5fd64fff3ef9b7d3096686', pr_url=None, repo_url=RepoUrl('https://huggingface.co/G20-CS4248/bert-baseline-qa', endpoint='https://huggingface.co', repo_type='model', repo_id='G20-CS4248/bert-baseline-qa'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aE53-_0mjL1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
